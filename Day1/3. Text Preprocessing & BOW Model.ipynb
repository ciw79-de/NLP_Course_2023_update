{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbcc8ad3",
   "metadata": {},
   "source": [
    "# Text Analysis and NLP fundamentals\n",
    "\n",
    "### Regular Expressions\n",
    "    - pythons in-build re module\n",
    "\n",
    "### CSV files\n",
    "    - working with csv files using pandas\n",
    "\n",
    "### Text preprocessing\n",
    "    - removal of unwanted characters\n",
    "    - text normalisation\n",
    "    - tokenisation\n",
    "    - stopword removal\n",
    "    - lemmatising and stemming\n",
    "    \n",
    "    \n",
    "### Bag-of-Words model\n",
    "    - one-hot-encoded vectors\n",
    "    - limitations\n",
    "\n",
    "\n",
    "NLP & ML libraries:\n",
    "- NLTK: https://www.nltk.org/\n",
    "- spacy: https://spacy.io/\n",
    "- scikit-learn: https://scikit-learn.org/represented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2e862a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2739fa",
   "metadata": {},
   "source": [
    "## Regular Expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb35863",
   "metadata": {},
   "source": [
    "A regular expression (shortened as regex or regexp)is a sequence of characters that specifies a search pattern in text. Usually such patterns are used by string-searching algorithms for \"find\" or \"find and replace\" operations on strings, or for input validation.\n",
    "\n",
    "You can try out all the examples given below here: https://www.w3schools.com/python/python_regex.asp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a957d1a9",
   "metadata": {},
   "source": [
    "The re module offers a set of functions that allows us to search a string for a match:\n",
    "\n",
    "| Function     | Description              |\n",
    "|--------------|--------------------------|\n",
    "| findall      | Returns a list containing all matches  | \n",
    "| search       |  \tReturns a Match object if there is a match anywhere in the string       |\n",
    "| split         | Returns a list where the string has been split at each match  |\n",
    "| sub    | Replaces one or many matches with a string          | \n",
    "| finditer  | Returns an iterator yielding MatchObject instances over all non-overlapping matches for the RE pattern in string  |  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed97565",
   "metadata": {},
   "source": [
    "### Metacharacters\n",
    "Metacharacters are characters with a special meaning:\n",
    "\n",
    "| Character     | Description              |   Example                       |\n",
    "|--------------|--------------------------|--------------------------|\n",
    "|  []     | A set of characters  | \"[a-m]\"  | \n",
    "|  \\      | Signals a special sequence (can also be used to escape special characters)   |  \"\\d\"   |\n",
    "|  .   | Any character (except newline character) | \"he..o\" | \n",
    "|  ^   |  Starts with   |  \"^hello\"  | \n",
    "|  $   |   \tEnds with   | \"planet\\$\"   | \n",
    "|  *   |  Zero or more occurrences   | \t\"he.*o\"  | \n",
    "|  +   |   \tOne or more occurrences   |   \t\"he.+o\"  | \n",
    "|  ?   |  Zero or one occurrences   |  \t\"he.?o\"   | \n",
    "|  {}   | Exactly the specified number of occurrences    |   \t\"he.{2}o\"  | \n",
    "|  \\|  |    \tEither or  |  \"falls|stays\"  | \n",
    "|  ()   | Capture and group    | a(bc)+   | "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277819c3",
   "metadata": {},
   "source": [
    "### Special Sequences\n",
    "\n",
    "A special sequence is a \\ followed by one of the characters in the list below, and has a special meaning:\n",
    "\n",
    "| Function     | Description              |  Example                 |\n",
    "|--------------|--------------------------|--------------------------|\n",
    "|  \\A     |   \tReturns a match if the specified characters are at the beginning of the string |  \t\"\\AThe\"  | \n",
    "|  \\b      |  Returns a match where the specified characters are at the beginning or at the end of a word* |  r\"\\bain\"  |\n",
    "|  \\B   | Returns a match where the specified characters are present, but NOT at the beginning (or at the end) of a word  | r\"\\Bain\" | \n",
    "|  \\d  |  Returns a match where the string contains digits (numbers from 0-9)   | \"\\d\"   | \n",
    "|  \\D   | Returns a match where the string DOES NOT contain digits | \"\\D\" | \n",
    "|  \\s   |   \tReturns a match where the string contains a white space character   | \"\\s\"   | \n",
    "|  \\S   |  \tReturns a match where the string DOES NOT contain a white space character | \"\\S\" | \n",
    "|  \\w   | Returns a match where the string contains any word characters (characters from a to Z, digits from 0-9, and the underscore _ character)    |  \"\\w\"  | \n",
    "|  \\W   |  \tReturns a match where the string DOES NOT contain any word characters    | \"\\W\"   |\n",
    "|  \\Z   |  \tReturns a match if the specified characters are at the end of the string    | \"Spain\\Z\"   |\n",
    "\n",
    "*(the \"r\" in the beginning is making sure that the string is being treated as a \"raw string\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855ae872",
   "metadata": {},
   "source": [
    "### Sets\n",
    "\n",
    "A set is a set of characters inside a pair of square brackets [] with a special meaning:\n",
    "\n",
    "| Set    | Description              | \n",
    "|--------------|--------------------------|\n",
    "|   [arn]      |   \tReturns a match where one of the specified characters (a, r, or n) are present              | \n",
    "|   [a-n]           |  \tReturns a match for any lower case character, alphabetically between a and n              | \n",
    "|   [^arn]           |  \tReturns a match for any character EXCEPT a, r, and n              | \n",
    "|   [0123]           | Returns a match where any of the specified digits (0, 1, 2, or 3) are present              | \n",
    "|   [0-9]           |   \tReturns a match for any digit between 0 and 9             | \n",
    "|   [0-5][0-9]           |     \tReturns a match for any two-digit numbers from 00 and 59           | \n",
    "|   [a-zA-Z]           |   \tReturns a match for any character alphabetically between a and z, lower case OR upper case             | \n",
    "| [Cc]at \t | Returns `Cat` or `cat`  | \n",
    "|     [+]         |  In sets, +, *, ., \\|, (), \\$,\\{\\} has no special meaning, so \\[+\\] means: return a match for any + character in the string             | "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd038c9",
   "metadata": {},
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1295cd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89f4ac77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['UK', 'EU', 'EU']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "#finding all instances\n",
    "txt = \"The UK has left the EU. The EU now only has 27 member states.\"\n",
    "x = re.findall(r\"EU|UK\", txt)\n",
    "print(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "549d48d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(20, 22), match='EU'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getting the indices\n",
    "x = re.search(\"EU\", txt) #only finds first occurance\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd2bc7b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 22)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.span()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "465b7948",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'EU'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt[20:22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04ee001a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(20, 22), (28, 30)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#finding all indices with re.finditer\n",
    "[(m.start(0), m.end(0)) for m in re.finditer(\"EU\", txt)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89042ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['UK', 'EU', 'EU', 'UK']\n"
     ]
    }
   ],
   "source": [
    "#need to specify word boundaries to avoid also matching EU in EUROZONE\n",
    "txt = \"The UK has left the EU. The EU now only has 27 member states. The UK had never been in the EUROZONE.\"\n",
    "x = re.findall(r\"\\bEU\\b|\\bUK\\b\", txt)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cac35f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5c3e6c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The UK has left the EU', ' The EU now only has 27 member states', ' The UK had never been in the EUROZONE', '']\n"
     ]
    }
   ],
   "source": [
    "#splitting\n",
    "x = re.split(\"\\.\", txt)   #dont forget that we need to escape \".\"\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5fdf4cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The United Kingdom has left the EU. The EU now only has 27 member states. The United Kingdom had never been in the EUROZONE.\n"
     ]
    }
   ],
   "source": [
    "#substitution\n",
    "x = re.sub(\"UK\", \"United Kingdom\", txt)\n",
    "print(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427e8554",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e7edbf89",
   "metadata": {},
   "source": [
    "## Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599d4dc9",
   "metadata": {},
   "source": [
    "For handling large or multi-column CSV data sets there is a nice Python module called pandas. It is a module for conveniently managing big data with multiple features (check the official module documentation at https://pandas.pydata.org/docs/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81672819",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a85dcc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"dummy_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "29d35ff1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>text</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sms</td>\n",
       "      <td>Wa, ur openin sentence very formal... Anyway, ...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sms</td>\n",
       "      <td>As I entered my cabin my PA said, '' Happy B'd...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sms</td>\n",
       "      <td>here is my new address -apples&amp;pairs&amp;all that ...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sms</td>\n",
       "      <td>Goodo! Yes we must speak friday - egg-potato r...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sms</td>\n",
       "      <td>Hmm...my uncle just informed me that he's payi...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>sms</td>\n",
       "      <td>PRIVATE! Your 2004 Account Statement for 07742...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sms</td>\n",
       "      <td>URGENT! Your Mobile No. was awarded σú2000 Bon...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sms</td>\n",
       "      <td>GENT! We are trying to contact you. Last weeke...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sms</td>\n",
       "      <td>You are a winner U have been specially selecte...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sms</td>\n",
       "      <td>Todays Voda numbers ending 7548 are selected t...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>sms</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>review</td>\n",
       "      <td>I WAS SO DISAPPOINTED!! ABSOLUTELY AWEFUL SERV...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>review</td>\n",
       "      <td>the food was good</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>review</td>\n",
       "      <td>THE FOOD WAS SO GOOD!!!!!! &lt;3&lt;3</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>review</td>\n",
       "      <td>not very nice staff, food was ok</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>review</td>\n",
       "      <td>I didn’t like it at all – food was cold and st...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>review</td>\n",
       "      <td>BEST PLACE EVER</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>review</td>\n",
       "      <td>my friends liked it but I not so much</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>review</td>\n",
       "      <td>I liked the food but the music was too loud an...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>review</td>\n",
       "      <td>:):):) like!</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>review</td>\n",
       "      <td>waiting time too long, table too small, overal...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>news_article</td>\n",
       "      <td>worldcom ex-boss launches defence lawyers defe...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>news_article</td>\n",
       "      <td>german business confidence slides german busin...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>news_article</td>\n",
       "      <td>bbc poll indicates economic gloom citizens in ...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>news_article</td>\n",
       "      <td>lifestyle  governs mobile choice  faster  bett...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>news_article</td>\n",
       "      <td>enron bosses in $168m payout eighteen former e...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>news_article</td>\n",
       "      <td>howard  truanted to play snooker  conservative...</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>news_article</td>\n",
       "      <td>wales silent on grand slam talk rhys williams ...</td>\n",
       "      <td>sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>news_article</td>\n",
       "      <td>french honour for director parker british film...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>news_article</td>\n",
       "      <td>car giant hit by mercedes slump a slump in pro...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>news_article</td>\n",
       "      <td>fockers fuel festive film chart comedy meet th...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            type                                               text  \\\n",
       "0            sms  Wa, ur openin sentence very formal... Anyway, ...   \n",
       "1            sms  As I entered my cabin my PA said, '' Happy B'd...   \n",
       "2            sms  here is my new address -apples&pairs&all that ...   \n",
       "3            sms  Goodo! Yes we must speak friday - egg-potato r...   \n",
       "4            sms  Hmm...my uncle just informed me that he's payi...   \n",
       "5            sms  PRIVATE! Your 2004 Account Statement for 07742...   \n",
       "6            sms  URGENT! Your Mobile No. was awarded σú2000 Bon...   \n",
       "7            sms  GENT! We are trying to contact you. Last weeke...   \n",
       "8            sms  You are a winner U have been specially selecte...   \n",
       "9            sms  Todays Voda numbers ending 7548 are selected t...   \n",
       "10           sms                                                NaN   \n",
       "11        review  I WAS SO DISAPPOINTED!! ABSOLUTELY AWEFUL SERV...   \n",
       "12        review                                  the food was good   \n",
       "13        review                    THE FOOD WAS SO GOOD!!!!!! <3<3   \n",
       "14        review                   not very nice staff, food was ok   \n",
       "15        review  I didn’t like it at all – food was cold and st...   \n",
       "16        review                                    BEST PLACE EVER   \n",
       "17        review              my friends liked it but I not so much   \n",
       "18        review  I liked the food but the music was too loud an...   \n",
       "19        review                                       :):):) like!   \n",
       "20        review  waiting time too long, table too small, overal...   \n",
       "21  news_article  worldcom ex-boss launches defence lawyers defe...   \n",
       "22  news_article  german business confidence slides german busin...   \n",
       "23  news_article  bbc poll indicates economic gloom citizens in ...   \n",
       "24  news_article  lifestyle  governs mobile choice  faster  bett...   \n",
       "25  news_article  enron bosses in $168m payout eighteen former e...   \n",
       "26  news_article  howard  truanted to play snooker  conservative...   \n",
       "27  news_article  wales silent on grand slam talk rhys williams ...   \n",
       "28  news_article  french honour for director parker british film...   \n",
       "29  news_article  car giant hit by mercedes slump a slump in pro...   \n",
       "30  news_article  fockers fuel festive film chart comedy meet th...   \n",
       "\n",
       "              tag  \n",
       "0             ham  \n",
       "1             ham  \n",
       "2             ham  \n",
       "3             ham  \n",
       "4             ham  \n",
       "5            spam  \n",
       "6            spam  \n",
       "7            spam  \n",
       "8            spam  \n",
       "9            spam  \n",
       "10            ham  \n",
       "11       negative  \n",
       "12       positive  \n",
       "13       positive  \n",
       "14        neutral  \n",
       "15       negative  \n",
       "16       positive  \n",
       "17        neutral  \n",
       "18        neutral  \n",
       "19       positive  \n",
       "20       negative  \n",
       "21       business  \n",
       "22       business  \n",
       "23       business  \n",
       "24           tech  \n",
       "25       business  \n",
       "26       politics  \n",
       "27          sport  \n",
       "28  entertainment  \n",
       "29       business  \n",
       "30  entertainment  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6963af72",
   "metadata": {},
   "source": [
    "As you can see pandas creates a table containing columns and rows. This table object is called DataFrame and it consists of columns called Series. Every entry is presented as a row.\n",
    "\n",
    "If you deal with a very large data set you probably don't want to print all entries, you rather want to check if the file loaded properly. To do so, you can apply the .head() method on the newly created DataFrame object.\n",
    "\n",
    "If you are not familiar with `pandas`, you can treat it as a more powerful MS Excel, since you can manipulate or process all the data using Python. Let's say you want to see all `text` fields of entries being classified as the `sms`. Additionally, you want these text fields formatted in lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b91821d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sms_df = df[df['type'] == 'sms']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3db2b46e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>text</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sms</td>\n",
       "      <td>Wa, ur openin sentence very formal... Anyway, ...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sms</td>\n",
       "      <td>As I entered my cabin my PA said, '' Happy B'd...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sms</td>\n",
       "      <td>here is my new address -apples&amp;pairs&amp;all that ...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sms</td>\n",
       "      <td>Goodo! Yes we must speak friday - egg-potato r...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sms</td>\n",
       "      <td>Hmm...my uncle just informed me that he's payi...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>sms</td>\n",
       "      <td>PRIVATE! Your 2004 Account Statement for 07742...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sms</td>\n",
       "      <td>URGENT! Your Mobile No. was awarded σú2000 Bon...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sms</td>\n",
       "      <td>GENT! We are trying to contact you. Last weeke...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sms</td>\n",
       "      <td>You are a winner U have been specially selecte...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sms</td>\n",
       "      <td>Todays Voda numbers ending 7548 are selected t...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>sms</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                               text   tag\n",
       "0   sms  Wa, ur openin sentence very formal... Anyway, ...   ham\n",
       "1   sms  As I entered my cabin my PA said, '' Happy B'd...   ham\n",
       "2   sms  here is my new address -apples&pairs&all that ...   ham\n",
       "3   sms  Goodo! Yes we must speak friday - egg-potato r...   ham\n",
       "4   sms  Hmm...my uncle just informed me that he's payi...   ham\n",
       "5   sms  PRIVATE! Your 2004 Account Statement for 07742...  spam\n",
       "6   sms  URGENT! Your Mobile No. was awarded σú2000 Bon...  spam\n",
       "7   sms  GENT! We are trying to contact you. Last weeke...  spam\n",
       "8   sms  You are a winner U have been specially selecte...  spam\n",
       "9   sms  Todays Voda numbers ending 7548 are selected t...  spam\n",
       "10  sms                                                NaN   ham"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6c3e137a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>text</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sms</td>\n",
       "      <td>wa, ur openin sentence very formal... anyway, ...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sms</td>\n",
       "      <td>as i entered my cabin my pa said, '' happy b'd...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sms</td>\n",
       "      <td>here is my new address -apples&amp;pairs&amp;all that ...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sms</td>\n",
       "      <td>goodo! yes we must speak friday - egg-potato r...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sms</td>\n",
       "      <td>hmm...my uncle just informed me that he's payi...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>sms</td>\n",
       "      <td>private! your 2004 account statement for 07742...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sms</td>\n",
       "      <td>urgent! your mobile no. was awarded σú2000 bon...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sms</td>\n",
       "      <td>gent! we are trying to contact you. last weeke...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sms</td>\n",
       "      <td>you are a winner u have been specially selecte...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sms</td>\n",
       "      <td>todays voda numbers ending 7548 are selected t...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>sms</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                               text   tag\n",
       "0   sms  wa, ur openin sentence very formal... anyway, ...   ham\n",
       "1   sms  as i entered my cabin my pa said, '' happy b'd...   ham\n",
       "2   sms  here is my new address -apples&pairs&all that ...   ham\n",
       "3   sms  goodo! yes we must speak friday - egg-potato r...   ham\n",
       "4   sms  hmm...my uncle just informed me that he's payi...   ham\n",
       "5   sms  private! your 2004 account statement for 07742...  spam\n",
       "6   sms  urgent! your mobile no. was awarded σú2000 bon...  spam\n",
       "7   sms  gent! we are trying to contact you. last weeke...  spam\n",
       "8   sms  you are a winner u have been specially selecte...  spam\n",
       "9   sms  todays voda numbers ending 7548 are selected t...  spam\n",
       "10  sms                                                NaN   ham"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms_df[\"text\"] = sms_df['text'].str.lower()  # Equivalent to sms_df.text.str.lower()\n",
    "sms_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f34516",
   "metadata": {},
   "source": [
    "We can see that the text is missing in one of the rows. So this row is no use for us. Let's drop it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94de61cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sms_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5acb51f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sms_df = sms_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fb29d182",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>text</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sms</td>\n",
       "      <td>wa, ur openin sentence very formal... anyway, ...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sms</td>\n",
       "      <td>as i entered my cabin my pa said, '' happy b'd...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sms</td>\n",
       "      <td>here is my new address -apples&amp;pairs&amp;all that ...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sms</td>\n",
       "      <td>goodo! yes we must speak friday - egg-potato r...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sms</td>\n",
       "      <td>hmm...my uncle just informed me that he's payi...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>sms</td>\n",
       "      <td>private! your 2004 account statement for 07742...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sms</td>\n",
       "      <td>urgent! your mobile no. was awarded σú2000 bon...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sms</td>\n",
       "      <td>gent! we are trying to contact you. last weeke...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sms</td>\n",
       "      <td>you are a winner u have been specially selecte...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sms</td>\n",
       "      <td>todays voda numbers ending 7548 are selected t...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  type                                               text   tag\n",
       "0  sms  wa, ur openin sentence very formal... anyway, ...   ham\n",
       "1  sms  as i entered my cabin my pa said, '' happy b'd...   ham\n",
       "2  sms  here is my new address -apples&pairs&all that ...   ham\n",
       "3  sms  goodo! yes we must speak friday - egg-potato r...   ham\n",
       "4  sms  hmm...my uncle just informed me that he's payi...   ham\n",
       "5  sms  private! your 2004 account statement for 07742...  spam\n",
       "6  sms  urgent! your mobile no. was awarded σú2000 bon...  spam\n",
       "7  sms  gent! we are trying to contact you. last weeke...  spam\n",
       "8  sms  you are a winner u have been specially selecte...  spam\n",
       "9  sms  todays voda numbers ending 7548 are selected t...  spam"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fe351f",
   "metadata": {},
   "source": [
    "Now, let's try to add a new column containing the length of the messages in the `sms_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "54c285c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>text</th>\n",
       "      <th>tag</th>\n",
       "      <th>text_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sms</td>\n",
       "      <td>wa, ur openin sentence very formal... anyway, ...</td>\n",
       "      <td>ham</td>\n",
       "      <td>143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sms</td>\n",
       "      <td>as i entered my cabin my pa said, '' happy b'd...</td>\n",
       "      <td>ham</td>\n",
       "      <td>156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sms</td>\n",
       "      <td>here is my new address -apples&amp;pairs&amp;all that ...</td>\n",
       "      <td>ham</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sms</td>\n",
       "      <td>goodo! yes we must speak friday - egg-potato r...</td>\n",
       "      <td>ham</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sms</td>\n",
       "      <td>hmm...my uncle just informed me that he's payi...</td>\n",
       "      <td>ham</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>sms</td>\n",
       "      <td>private! your 2004 account statement for 07742...</td>\n",
       "      <td>spam</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sms</td>\n",
       "      <td>urgent! your mobile no. was awarded σú2000 bon...</td>\n",
       "      <td>spam</td>\n",
       "      <td>157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sms</td>\n",
       "      <td>gent! we are trying to contact you. last weeke...</td>\n",
       "      <td>spam</td>\n",
       "      <td>159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sms</td>\n",
       "      <td>you are a winner u have been specially selecte...</td>\n",
       "      <td>spam</td>\n",
       "      <td>153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sms</td>\n",
       "      <td>todays voda numbers ending 7548 are selected t...</td>\n",
       "      <td>spam</td>\n",
       "      <td>156</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  type                                               text   tag  text_length\n",
       "0  sms  wa, ur openin sentence very formal... anyway, ...   ham          143\n",
       "1  sms  as i entered my cabin my pa said, '' happy b'd...   ham          156\n",
       "2  sms  here is my new address -apples&pairs&all that ...   ham           53\n",
       "3  sms  goodo! yes we must speak friday - egg-potato r...   ham           72\n",
       "4  sms  hmm...my uncle just informed me that he's payi...   ham           86\n",
       "5  sms  private! your 2004 account statement for 07742...  spam          144\n",
       "6  sms  urgent! your mobile no. was awarded σú2000 bon...  spam          157\n",
       "7  sms  gent! we are trying to contact you. last weeke...  spam          159\n",
       "8  sms  you are a winner u have been specially selecte...  spam          153\n",
       "9  sms  todays voda numbers ending 7548 are selected t...  spam          156"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This line (creates) a new column and fills it with a length (.str.len()) of every message. \n",
    "sms_df['text_length'] = sms_df.text.str.len()\n",
    "sms_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f34fbf",
   "metadata": {},
   "source": [
    "You can see that the data is not shuffled - this can be problematic later, so let's shuffle it. The idiomatic way to do this with Pandas is to use the .sample method of your data frame to sample all rows without replacement. The frac keyword argument specifies the fraction of rows to return in the random sample, so frac=1 means to return all rows (in random order)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "69b5de27",
   "metadata": {},
   "outputs": [],
   "source": [
    "sms_df = sms_df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3ebb791c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>text</th>\n",
       "      <th>tag</th>\n",
       "      <th>text_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sms</td>\n",
       "      <td>urgent! your mobile no. was awarded σú2000 bon...</td>\n",
       "      <td>spam</td>\n",
       "      <td>157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sms</td>\n",
       "      <td>wa, ur openin sentence very formal... anyway, ...</td>\n",
       "      <td>ham</td>\n",
       "      <td>143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sms</td>\n",
       "      <td>gent! we are trying to contact you. last weeke...</td>\n",
       "      <td>spam</td>\n",
       "      <td>159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sms</td>\n",
       "      <td>as i entered my cabin my pa said, '' happy b'd...</td>\n",
       "      <td>ham</td>\n",
       "      <td>156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sms</td>\n",
       "      <td>goodo! yes we must speak friday - egg-potato r...</td>\n",
       "      <td>ham</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sms</td>\n",
       "      <td>you are a winner u have been specially selecte...</td>\n",
       "      <td>spam</td>\n",
       "      <td>153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sms</td>\n",
       "      <td>here is my new address -apples&amp;pairs&amp;all that ...</td>\n",
       "      <td>ham</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>sms</td>\n",
       "      <td>private! your 2004 account statement for 07742...</td>\n",
       "      <td>spam</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sms</td>\n",
       "      <td>hmm...my uncle just informed me that he's payi...</td>\n",
       "      <td>ham</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sms</td>\n",
       "      <td>todays voda numbers ending 7548 are selected t...</td>\n",
       "      <td>spam</td>\n",
       "      <td>156</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  type                                               text   tag  text_length\n",
       "6  sms  urgent! your mobile no. was awarded σú2000 bon...  spam          157\n",
       "0  sms  wa, ur openin sentence very formal... anyway, ...   ham          143\n",
       "7  sms  gent! we are trying to contact you. last weeke...  spam          159\n",
       "1  sms  as i entered my cabin my pa said, '' happy b'd...   ham          156\n",
       "3  sms  goodo! yes we must speak friday - egg-potato r...   ham           72\n",
       "8  sms  you are a winner u have been specially selecte...  spam          153\n",
       "2  sms  here is my new address -apples&pairs&all that ...   ham           53\n",
       "5  sms  private! your 2004 account statement for 07742...  spam          144\n",
       "4  sms  hmm...my uncle just informed me that he's payi...   ham           86\n",
       "9  sms  todays voda numbers ending 7548 are selected t...  spam          156"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "75ba7d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#If you wish to reset the index, you could do e.g.\n",
    "sms_df = sms_df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9ea87df4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>text</th>\n",
       "      <th>tag</th>\n",
       "      <th>text_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sms</td>\n",
       "      <td>hmm...my uncle just informed me that he's payi...</td>\n",
       "      <td>ham</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sms</td>\n",
       "      <td>todays voda numbers ending 7548 are selected t...</td>\n",
       "      <td>spam</td>\n",
       "      <td>156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sms</td>\n",
       "      <td>private! your 2004 account statement for 07742...</td>\n",
       "      <td>spam</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sms</td>\n",
       "      <td>you are a winner u have been specially selecte...</td>\n",
       "      <td>spam</td>\n",
       "      <td>153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sms</td>\n",
       "      <td>as i entered my cabin my pa said, '' happy b'd...</td>\n",
       "      <td>ham</td>\n",
       "      <td>156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>sms</td>\n",
       "      <td>goodo! yes we must speak friday - egg-potato r...</td>\n",
       "      <td>ham</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sms</td>\n",
       "      <td>here is my new address -apples&amp;pairs&amp;all that ...</td>\n",
       "      <td>ham</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sms</td>\n",
       "      <td>wa, ur openin sentence very formal... anyway, ...</td>\n",
       "      <td>ham</td>\n",
       "      <td>143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sms</td>\n",
       "      <td>gent! we are trying to contact you. last weeke...</td>\n",
       "      <td>spam</td>\n",
       "      <td>159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sms</td>\n",
       "      <td>urgent! your mobile no. was awarded σú2000 bon...</td>\n",
       "      <td>spam</td>\n",
       "      <td>157</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  type                                               text   tag  text_length\n",
       "0  sms  hmm...my uncle just informed me that he's payi...   ham           86\n",
       "1  sms  todays voda numbers ending 7548 are selected t...  spam          156\n",
       "2  sms  private! your 2004 account statement for 07742...  spam          144\n",
       "3  sms  you are a winner u have been specially selecte...  spam          153\n",
       "4  sms  as i entered my cabin my pa said, '' happy b'd...   ham          156\n",
       "5  sms  goodo! yes we must speak friday - egg-potato r...   ham           72\n",
       "6  sms  here is my new address -apples&pairs&all that ...   ham           53\n",
       "7  sms  wa, ur openin sentence very formal... anyway, ...   ham          143\n",
       "8  sms  gent! we are trying to contact you. last weeke...  spam          159\n",
       "9  sms  urgent! your mobile no. was awarded σú2000 bon...  spam          157"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "728ee7ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>text</th>\n",
       "      <th>tag</th>\n",
       "      <th>text_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sms</td>\n",
       "      <td>gent! we are trying to contact you. last weeke...</td>\n",
       "      <td>spam</td>\n",
       "      <td>159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sms</td>\n",
       "      <td>urgent! your mobile no. was awarded σú2000 bon...</td>\n",
       "      <td>spam</td>\n",
       "      <td>157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sms</td>\n",
       "      <td>todays voda numbers ending 7548 are selected t...</td>\n",
       "      <td>spam</td>\n",
       "      <td>156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sms</td>\n",
       "      <td>as i entered my cabin my pa said, '' happy b'd...</td>\n",
       "      <td>ham</td>\n",
       "      <td>156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sms</td>\n",
       "      <td>you are a winner u have been specially selecte...</td>\n",
       "      <td>spam</td>\n",
       "      <td>153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sms</td>\n",
       "      <td>private! your 2004 account statement for 07742...</td>\n",
       "      <td>spam</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sms</td>\n",
       "      <td>wa, ur openin sentence very formal... anyway, ...</td>\n",
       "      <td>ham</td>\n",
       "      <td>143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sms</td>\n",
       "      <td>hmm...my uncle just informed me that he's payi...</td>\n",
       "      <td>ham</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>sms</td>\n",
       "      <td>goodo! yes we must speak friday - egg-potato r...</td>\n",
       "      <td>ham</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sms</td>\n",
       "      <td>here is my new address -apples&amp;pairs&amp;all that ...</td>\n",
       "      <td>ham</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  type                                               text   tag  text_length\n",
       "8  sms  gent! we are trying to contact you. last weeke...  spam          159\n",
       "9  sms  urgent! your mobile no. was awarded σú2000 bon...  spam          157\n",
       "1  sms  todays voda numbers ending 7548 are selected t...  spam          156\n",
       "4  sms  as i entered my cabin my pa said, '' happy b'd...   ham          156\n",
       "3  sms  you are a winner u have been specially selecte...  spam          153\n",
       "2  sms  private! your 2004 account statement for 07742...  spam          144\n",
       "7  sms  wa, ur openin sentence very formal... anyway, ...   ham          143\n",
       "0  sms  hmm...my uncle just informed me that he's payi...   ham           86\n",
       "5  sms  goodo! yes we must speak friday - egg-potato r...   ham           72\n",
       "6  sms  here is my new address -apples&pairs&all that ...   ham           53"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sorting based on text_length\n",
    "sms_df.sort_values('text_length', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8b0c0f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "length_series = sms_df.text_length.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e53fe718",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[86, 156, 144, 153, 156, 72, 53, 143, 159, 157]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "47a60cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "sms_df = sms_df.drop(\"text_length\", axis=1) #0 is for lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c81aa266",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>text</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sms</td>\n",
       "      <td>hmm...my uncle just informed me that he's payi...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sms</td>\n",
       "      <td>todays voda numbers ending 7548 are selected t...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sms</td>\n",
       "      <td>private! your 2004 account statement for 07742...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sms</td>\n",
       "      <td>you are a winner u have been specially selecte...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sms</td>\n",
       "      <td>as i entered my cabin my pa said, '' happy b'd...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>sms</td>\n",
       "      <td>goodo! yes we must speak friday - egg-potato r...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sms</td>\n",
       "      <td>here is my new address -apples&amp;pairs&amp;all that ...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sms</td>\n",
       "      <td>wa, ur openin sentence very formal... anyway, ...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sms</td>\n",
       "      <td>gent! we are trying to contact you. last weeke...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sms</td>\n",
       "      <td>urgent! your mobile no. was awarded σú2000 bon...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  type                                               text   tag\n",
       "0  sms  hmm...my uncle just informed me that he's payi...   ham\n",
       "1  sms  todays voda numbers ending 7548 are selected t...  spam\n",
       "2  sms  private! your 2004 account statement for 07742...  spam\n",
       "3  sms  you are a winner u have been specially selecte...  spam\n",
       "4  sms  as i entered my cabin my pa said, '' happy b'd...   ham\n",
       "5  sms  goodo! yes we must speak friday - egg-potato r...   ham\n",
       "6  sms  here is my new address -apples&pairs&all that ...   ham\n",
       "7  sms  wa, ur openin sentence very formal... anyway, ...   ham\n",
       "8  sms  gent! we are trying to contact you. last weeke...  spam\n",
       "9  sms  urgent! your mobile no. was awarded σú2000 bon...  spam"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e8df1389",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>text</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sms</td>\n",
       "      <td>hmm...my uncle just informed me that he's payi...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sms</td>\n",
       "      <td>todays voda numbers ending 7548 are selected t...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sms</td>\n",
       "      <td>private! your 2004 account statement for 07742...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sms</td>\n",
       "      <td>you are a winner u have been specially selecte...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sms</td>\n",
       "      <td>as i entered my cabin my pa said, '' happy b'd...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>sms</td>\n",
       "      <td>goodo! yes we must speak friday - egg-potato r...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sms</td>\n",
       "      <td>here is my new address -apples&amp;pairs&amp;all that ...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sms</td>\n",
       "      <td>wa, ur openin sentence very formal... anyway, ...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sms</td>\n",
       "      <td>gent! we are trying to contact you. last weeke...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sms</td>\n",
       "      <td>urgent! your mobile no. was awarded σú2000 bon...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  type                                               text   tag\n",
       "0  sms  hmm...my uncle just informed me that he's payi...   ham\n",
       "1  sms  todays voda numbers ending 7548 are selected t...  spam\n",
       "2  sms  private! your 2004 account statement for 07742...  spam\n",
       "3  sms  you are a winner u have been specially selecte...  spam\n",
       "4  sms  as i entered my cabin my pa said, '' happy b'd...   ham\n",
       "5  sms  goodo! yes we must speak friday - egg-potato r...   ham\n",
       "6  sms  here is my new address -apples&pairs&all that ...   ham\n",
       "7  sms  wa, ur openin sentence very formal... anyway, ...   ham\n",
       "8  sms  gent! we are trying to contact you. last weeke...  spam\n",
       "9  sms  urgent! your mobile no. was awarded σú2000 bon...  spam"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ab0c2e33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ham', 'spam'], dtype=object)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#other useful functions\n",
    "\n",
    "sms_df.tag.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "55cc5823",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     5\n",
       "spam    5\n",
       "Name: tag, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms_df.tag.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "23ed59c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sms_df.tag.value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427f9cbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd5d04d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c5b5d607",
   "metadata": {},
   "source": [
    "## Text Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6543ff",
   "metadata": {},
   "source": [
    "### Removal of unwanted characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd37fe0",
   "metadata": {},
   "source": [
    "The is a primary step in the process of text cleaning. If we scrap some text from HTML/XML sources, we’ll need to get rid of all the tags, HTML entities, punctuation, non-alphabets, and any other kind of characters that might not be a part of the language. The general methods of such cleaning involve **regular expressions**, which can be used to filter out most of the unwanted texts.\n",
    "\n",
    "However, sometimes, depending on the type of data, we want to retain certain types of punctuation. Consider for example human-generated tweets which you want to classify as very angry, angry, neutral, happy, and very happy. Simple sentiment analysis might find it hard to differentiate between a happy, and very happy sentiment because the only difference between a happy and a very happy tweet might be punctuation.\n",
    "\n",
    "Example:\n",
    "\n",
    "*`This is amazing`* vs *`THIS IS AMAZING!!!!!`*\n",
    "\n",
    "Or what about this one\n",
    "\n",
    "*`I don't know :) <3`* vs *`I don't know :(((`*\n",
    "\n",
    "Now let's create a simple function that keeps only letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bde7fc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_letters_only(raw_text):\n",
    "    letters_only_text = re.sub(\"[^a-zA-Z]\", \" \", raw_text)\n",
    "    return letters_only_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0b4569e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sms_sample = \"\"\"***** CONGRATlations **** You won 2 tIckETs to Hamilton in \n",
    "NYC http://www.hamiltonbroadway.com/J?NaIOl/event   wORtH over $500.00...CALL \n",
    "555-477-8914 or send message to: hamilton@freetix.com to get tickets !!!\"\"\"\n",
    "\n",
    "review_sample = \"\"\" THIS FOOD AND STAFF WAS AMAZING!!!!! ABSOLUTELY LOVE THAT PLACE <3<3<3\"\"\"\n",
    "\n",
    "news_sample = \"\"\"worldcom ex-boss launches defence lawyers defending former worldcom chief bernie ebbers against a battery of fraud charges have called a company whistleblower as their first witness.  cynthia cooper  worldcom s ex-head of internal accounting  alerted directors to irregular accounting practices at the us telecoms giant in 2002. her warnings led to the collapse of the firm following the discovery of an $11bn (┬ú5.7bn) accounting fraud. mr ebbers has pleaded not guilty to charges of fraud and conspiracy.  prosecution lawyers have argued that mr ebbers orchestrated a series of accounting tricks at worldcom  ordering employees to hide expenses and inflate revenues to meet wall street earnings estimates. but ms cooper  who now runs her own consulting business  told a jury in new york on wednesday that external auditors arthur andersen had approved worldcom s accounting in early 2001 and 2002. she said andersen had given a  green light  to the procedures and practices used by worldcom. mr ebber s lawyers have said he was unaware of the fraud  arguing that auditors did not alert him to any problems.  ms cooper also said that during shareholder meetings mr ebbers often passed over technical questions to the company s finance chief  giving only  brief  answers himself. the prosecution s star witness  former worldcom financial chief scott sullivan  has said that mr ebbers ordered accounting adjustments at the firm  telling him to  hit our books . however  ms cooper said mr sullivan had not mentioned  anything uncomfortable  about worldcom s accounting during a 2001 audit committee meeting. mr ebbers could face a jail sentence of 85 years if convicted of all the charges he is facing. worldcom emerged from bankruptcy protection in 2004  and is now known as mci. last week  mci agreed to a buyout by verizon communications in a deal valued at $6.75bn.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f53308ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'      CONGRATlations      You won   tIckETs to Hamilton in  NYC http   www hamiltonbroadway com J NaIOl event   wORtH over           CALL               or send message to  hamilton freetix com to get tickets    '"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keep_letters_only(sms_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4b04c9",
   "metadata": {},
   "source": [
    "You can see that this is not ideal as this leaves us with a lot of random stuff like \"www\" and \"com\". We will get back to that later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "588ba894",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' THIS FOOD AND STAFF WAS AMAZING      ABSOLUTELY LOVE THAT PLACE       '"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keep_letters_only(review_sample) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3234be3",
   "metadata": {},
   "source": [
    "We don't lose any meaning, but as mentioned previously, keeping the exclamation marks might be useful if we want to distinguish between positive and VERY positive reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b25ced56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'worldcom ex boss launches defence lawyers defending former worldcom chief bernie ebbers against a battery of fraud charges have called a company whistleblower as their first witness   cynthia cooper  worldcom s ex head of internal accounting  alerted directors to irregular accounting practices at the us telecoms giant in       her warnings led to the collapse of the firm following the discovery of an    bn       bn  accounting fraud  mr ebbers has pleaded not guilty to charges of fraud and conspiracy   prosecution lawyers have argued that mr ebbers orchestrated a series of accounting tricks at worldcom  ordering employees to hide expenses and inflate revenues to meet wall street earnings estimates  but ms cooper  who now runs her own consulting business  told a jury in new york on wednesday that external auditors arthur andersen had approved worldcom s accounting in early      and       she said andersen had given a  green light  to the procedures and practices used by worldcom  mr ebber s lawyers have said he was unaware of the fraud  arguing that auditors did not alert him to any problems   ms cooper also said that during shareholder meetings mr ebbers often passed over technical questions to the company s finance chief  giving only  brief  answers himself  the prosecution s star witness  former worldcom financial chief scott sullivan  has said that mr ebbers ordered accounting adjustments at the firm  telling him to  hit our books   however  ms cooper said mr sullivan had not mentioned  anything uncomfortable  about worldcom s accounting during a      audit committee meeting  mr ebbers could face a jail sentence of    years if convicted of all the charges he is facing  worldcom emerged from bankruptcy protection in       and is now known as mci  last week  mci agreed to a buyout by verizon communications in a deal valued at      bn '"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keep_letters_only(news_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f94e89",
   "metadata": {},
   "source": [
    "For news articles that works perfectly fine as we do not lose any relevant information in this case since we want to classify by genre (sports, business, tech, etc.).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28550929",
   "metadata": {},
   "source": [
    "### Text Normalisation\n",
    "\n",
    "Recall our sms sample:\n",
    "\n",
    "`**** CONGRATlations **** You won 2 tIckETs to Hamilton in NYC http://www.hamiltonbroadway.com/J?NaIOl/event wORtH over $500.00...CALL 555-477-8914 or send message to: hamilton@freetix.com to get the tickets !!`\n",
    "\n",
    "No doubt that this as spam. But clearly, there's a lot going on here: phone numbers, emails, website URLs, money amounts, and gratuitous whitespace and punctuation. Some terms are randomly capitalized, others are in all-caps. Since these terms might show up in any one of the training examples in countless forms, we need a way to ensure each training example is on an equal footing via a preprocessing step called **normalisation**. \n",
    "\n",
    "To detect spam messages we don't want the computer to know or remember which email address or phone number was previously used in a spam message. We want the computer to understand **the pattern** of a spammy message. For example, if the message contains a lot of money amounts, words like \"congratulations\", \"you won\", AND an email address, it should be more likely to be considered spam. Again, we do not care what was the particular email address.'\n",
    "\n",
    "So instead of removing the following terms, for each training example, let's replace them with a specific string.\n",
    "\n",
    "- Replace email addresses with `emailaddr`\n",
    "- Replace URLs with `httpaddr`\n",
    "- Replace money symbols with `moneysymb`\n",
    "- Replace phone numbers with `phonenumbr`\n",
    "- Replace numbers with `numbr`\n",
    "- get rid of all other punctuations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "279b2691",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalisation_sms(raw_text):\n",
    "    cleaned = re.sub(r'\\b[\\w\\-.]+?@\\w+?\\.\\w{2,4}\\b', 'emailaddr', raw_text)\n",
    "    cleaned = re.sub(r'(http[s]?\\S+)|(\\w+\\.[A-Za-z]{2,4}\\S*)', 'httpaddr',\n",
    "                     cleaned)\n",
    "    cleaned = re.sub(r'£|\\$|\\€', 'moneysymb ', cleaned) #add whitespace\n",
    "    cleaned = re.sub(\n",
    "        r'\\b(\\+\\d{1,2}\\s)?\\d?[\\-(.]?\\d{3}\\)?[\\s.-]?\\d{3}[\\s.-]?\\d{4}\\b',\n",
    "        'phonenumbr', cleaned)\n",
    "    cleaned = re.sub(r'\\d+(\\.\\d+)?', 'numbr', cleaned)\n",
    "    letters_only_text = re.sub(\"[^a-zA-Z]\", \" \", cleaned)\n",
    "    return letters_only_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0bc8ef83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'      CONGRATlations      You won numbr tIckETs to Hamilton in  NYC httpaddr   wORtH over moneysymb numbr   CALL  phonenumbr or send message to  emailaddr to get tickets    '"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalisation_sms(sms_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d9f501",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c33b0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a526d0e7",
   "metadata": {},
   "source": [
    "### Tokenisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26a1cdb",
   "metadata": {},
   "source": [
    "Tokenisation is the process of splitting a sentence into words (tokens).\n",
    "\n",
    "As you remember, in the previous notebook we used the .split() method which may be helpful in this case. Let's see an easy example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5097ac75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "28d2b4a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'bad', 'day', 'in', 'London', 'is', 'still', 'better', 'than', 'a', 'bad', 'day', 'anywhere', 'else.', 'test']\n"
     ]
    }
   ],
   "source": [
    "print(\"A bad day in London is still better than a bad day anywhere else. test\".split())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb98e314",
   "metadata": {},
   "source": [
    "You can see that the full stop at the end of the sentence was left with the word \"else\". Of course, people are smart enough to understand that both tokens have the same meaning. However, computer algorithms looking for patterns will treat \"else\" and \"else.\" like different tokens.\n",
    "\n",
    "One could use regular expressions and try to write a function that splits the sentence above in the correct way (keep in mind that there are words where the fullstop (or other punctuations) belong to the word, like \"Mr.\" and \"o'clock\"). But this would be tedious and error-prone. Good there are libraries that alredy do the job for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "320e026f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/lisawork/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/lisawork/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/lisawork/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/lisawork/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk \n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d966ef5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'do', \"n't\", 'like', 'stormy', 'weather', 'after', '8', \"o'clock\", 'in', 'the', 'evening', '!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "print(word_tokenize(\"I don't like stormy weather after 8 o'clock in the evening!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4bcdd02f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " \"don't\",\n",
       " 'like',\n",
       " 'stormy',\n",
       " 'weather',\n",
       " 'after',\n",
       " '8',\n",
       " \"o'clock\",\n",
       " 'in',\n",
       " 'the',\n",
       " 'evening!']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"I don't like stormy weather after 8 o'clock in the evening!\".split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f872f1b5",
   "metadata": {},
   "source": [
    "As you can see `word_tokenizer` does exactly what we want! Nltk provides also different tokenizers for different types of input. Let's compare the `word_tokenize` with `TweetTokenizer`, which has been designed to work better with Twitter-type source texts (including hashtags, mentions, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8717c84d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hey', '@', 'everyone', ',', 'this', 'is', 'a', 'sample', '#', 'Twitter', 'text', 'containing', 'some', 'emojis', ':', ')', ')', ')', '!', '!', '!', 'Have', 'fun', '<', '3', '!']\n",
      "\n",
      "['Hey', '@everyone', ',', 'this', 'is', 'a', 'sample', '#Twitter', 'text', 'containing', 'some', 'emojis', ':)', ')', ')', '!', '!', '!', 'Have', 'fun', '<3', '!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "\n",
    "print(word_tokenize(\"Hey @everyone, this is a sample #Twitter text containing some emojis :))) !!! Have fun <3 !\"))\n",
    "print()\n",
    "print(tt.tokenize(\"Hey @everyone, this is a sample #Twitter text containing some emojis :))) !!! Have fun <3 !\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2a1469a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize,sent_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250f6c97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "04de167c",
   "metadata": {},
   "source": [
    "### Stopword removal\n",
    "\n",
    "Stopwords are the words that are used very frequently. Words like “of, are, the, it, is” are some examples of stopwords. In applications like document search engines and document classification, where keywords are more important than general terms, removing stopwords can be a good idea. However, when searching for song lyrics or quores, stopwords can be important. \n",
    "\n",
    "“To be, or not to be” - Stopwords in such phrases actually play an important role, and hence, should not be dropped.\n",
    "\n",
    "Another example is negation. \"not\" is contained in many stopword lists, but deleting \"not\" out of a negative review can make a positive out of it.\n",
    "\n",
    "There are two common approaches to removing the stopwords, and both are fairly straightforward. One way is to count all the word occurrences, and providing a threshold value on the count, and getting rid of all the terms/words occurring more than the specified threshold value. The other way is to have a predetermined list of stopwords, which can be removed from the list of tokens/tokenized sentences. In the beginning, the second one may be better, as determining thresholds can be quite difficult.\n",
    "\n",
    "NLTK comes with many corpora, including a stopword list. This list contains around 200 terms. However, you may want to use one that contains almost 600 terms: [http://www.ai.mit.edu/projects/jmlr/papers/volume5/lewis04a/a11-smart-stop-list/english.stop](http://www.ai.mit.edu/projects/jmlr/papers/volume5/lewis04a/a11-smart-stop-list/english.stop) (That is the one we were using in the previous notebooks, just with the apostrophes removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fc85d095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'as', 'able', 'about', 'above', 'according', 'accordingly', 'across', 'actually', 'after']\n"
     ]
    }
   ],
   "source": [
    "stop_words = []\n",
    "\n",
    "with open(\"stopword_file.txt\", 'r') as f:\n",
    "    stop_words.extend(f.read().splitlines())\n",
    "\n",
    "\n",
    "print(stop_words[:10])  # First 10 stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bf425a6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "571"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76e27c6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words_nltk = set(stopwords.words('english'))\n",
    "len(stop_words_nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d626c6e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1eba51e7",
   "metadata": {},
   "source": [
    "### Lemmatising and Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9290f219",
   "metadata": {},
   "source": [
    "Lemmatisation and stemming both refer to a process of reducing a word to its root. The difference is that stem might not be an actual word whereas, a lemma is an actual word. It’s a handy tool if you want to avoid treating different forms of the same word as different words, e.g. *love, loved, loving*\n",
    "\n",
    "**Lemmatising:** considered, considers, consider → “consider”\n",
    "\n",
    "**Stemming:** considered, considering, consider → “consid”\n",
    "\n",
    "In many applications, there may be no significant difference between lemmatising and stemming when training classifiers. However, the best way to find out how they work and when to use which solution is to try them! NLTK comes with many different in-built lemmatisers and stemmers, so just plug and play.\n",
    "\n",
    "A note of caution: WordNetLemmatizer requires a POS-tag. The default is set to \"noun\" and therefore doesn't work with other words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1dbc9b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "consid\n",
      "considers\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "verb = \"considers\"\n",
    "noun = \"apples\"\n",
    "\n",
    "stemmed_verb =  stemmer.stem(verb)\n",
    "lemmatised_verb = lemmatizer.lemmatize(verb) #, \"v\") \n",
    "\n",
    "stemmed_noun =  stemmer.stem(noun)\n",
    "lemmatised_noun = lemmatizer.lemmatize(noun)\n",
    "\n",
    "print(stemmed_verb)\n",
    "print(lemmatised_verb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "20182f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "appl\n",
      "apple\n"
     ]
    }
   ],
   "source": [
    "print(stemmed_noun)\n",
    "print(lemmatised_noun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f7de4a4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'some word include apple and consider'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#better to use spacy for lemmatising\n",
    "import spacy\n",
    "\n",
    "load_model = spacy.load('en_core_web_sm', disable = ['parser','ner'])\n",
    "\n",
    "My_text = \"some words including apples and considers\"\n",
    "\n",
    "doc = load_model(My_text)\n",
    "\n",
    "\" \".join([token.lemma_ for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796746a0",
   "metadata": {},
   "source": [
    "## Bag-of-Words model\n",
    "\n",
    "Ok, now that we know how to preprocess and tokenise our text, it is time to convert it into computer-readable vectors. This is called feature extraction. The **bag-of-words (BOW) model** is a popular and simple feature extraction technique. The intuition behind BOW is that two sentences are said to be similar if they contain a similar set of words.\n",
    "\n",
    "The general idea of the BOW model is to count how many times each word (*token*) from the dataset occurs in a given sentence/source. The simplest way of implementing this model is using Python dictionaries. Let's try!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d83bf122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'They': 1, 'like': 1, 'apples': 1}\n",
      "{'We': 1, 'like': 1, 'bananas': 1}\n"
     ]
    }
   ],
   "source": [
    "sentence1 = \"They like apples\"\n",
    "sentence2 = \"We like bananas\"\n",
    "\n",
    "sentence1_bag = {}\n",
    "sentence2_bag = {}\n",
    "\n",
    "def create_bag(text):\n",
    "    bag = {}\n",
    "    for token in text.split():\n",
    "        if token in bag:\n",
    "            bag[token] += 1\n",
    "        else:\n",
    "            bag[token] = 1\n",
    "    return bag\n",
    "\n",
    "sentence1_bag = create_bag(sentence1)\n",
    "sentence2_bag = create_bag(sentence2)\n",
    "print(sentence1_bag)\n",
    "print(sentence2_bag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8788fe5e",
   "metadata": {},
   "source": [
    "Ok, now the computer understands how many and which words make up each sentence but is it able to compare them? No, because there isn't any connection between those sentences (yet!). We have to develop a \"common denominator\" for both sentences so we can compare them. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c02efe",
   "metadata": {},
   "source": [
    "### One-Hot Vectors\n",
    "In the case of the BOW model, the solution is to create a *bag* of all used tokens and encode words using computer-readable **One-Hot Vectors**. How does it work? BOW constructs a dictionary of *m* unique words in the corpus (vocabulary) and converts each word into a sparse vector of size *m*, where all values are set to 0 apart from the index of that word in the vocabulary. We can also say that each word is a feature and that sentences consist of features. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dce28fb",
   "metadata": {},
   "source": [
    "In the case above there are five different words: \"They\", \"We\", \"like\", \"apples\", \"bananas\". We can encode them using a vector of length 5.\n",
    "\n",
    "| word    | associated vector |\n",
    "|---------|-------------------|\n",
    "| They    | [1,0,0,0,0]       |\n",
    "| We      | [0,1,0,0,0]       |\n",
    "| like    | [0,0,1,0,0]       |\n",
    "| apples  | [0,0,0,1,0]       |\n",
    "| bananas | [0,0,0,0,1]       |\n",
    "\n",
    "\n",
    "A sentence can be represented by adding the vectors together.\n",
    "\n",
    "For example: *They like apples* can be expressed as *They + like + apples* and using vectors: [1,0,0,0,0] + [0,0,1,0,0] + [0,0,0,1,0] = [1,0,1,1,0], hence the computer readable version of \"They like apples\" is [1,0,1,1,0]. \n",
    "\n",
    "| sentence           | associated sum of vectors |\n",
    "|--------------------|---------------------------|\n",
    "| They like apples   | [1,0,1,1,0]               |\n",
    "| We like apples     | [0,1,1,1,0]               |\n",
    "| We like bananas    | [0,1,1,0,1]               |\n",
    "| apples like They   | [1,0,1,1,0]               |\n",
    "\n",
    "What if there are more than 1 occurrence of the same token? There are different ways of handling that: max-pooling only counts whether a word is present, but not how many times. Sum pooling counts the number of occurrences of each word.\n",
    "\n",
    "| sentence                     | method            | associated sum of vectors |\n",
    "|------------------------------|-------------------|---------------------------|\n",
    "| They like like like apples   | max-pooling       |[1,0,1,1,0]                |\n",
    "| They like like like apples   | sum pooling       |[1,0,3,1,0]                |\n",
    "\n",
    "\n",
    "\n",
    "Now, how to implement it in Python? Of course, we could develop our own methods of words vectorisation but the `scikit-learn` package gives us a set of useful tools!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738eaa12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7a2e1cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CountVectorizer(binary=True, stop_words=&#x27;english&#x27;, token_pattern=&#x27;\\\\S+&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer(binary=True, stop_words=&#x27;english&#x27;, token_pattern=&#x27;\\\\S+&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "CountVectorizer(binary=True, stop_words='english', token_pattern='\\\\S+')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Corpus containing all sentences\n",
    "#corpus = [sentence1, sentence2]\n",
    "corpus = [\"This is a test % @\"]\n",
    "\n",
    "vectorizer = CountVectorizer(binary=True, stop_words=\"english\", min_df=1, ngram_range=(1,1), token_pattern=\"\\S+\")\n",
    "vectorizer.fit(corpus)  # vectorizer learns tokens (features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2574e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e4931692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['They like apples', 'We like bananas']\n",
      "[[0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "print(corpus)\n",
    "print(vectorizer.transform([\"We like bananas\"]).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "13839d59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'like': 2, 'apples': 0, 'bananas': 1}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80ccd80",
   "metadata": {},
   "source": [
    "It is worth reading the documentation for CountVectorizer: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "\n",
    "by default, punctuation is ignored and everything is lowercased."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054cc311",
   "metadata": {},
   "source": [
    "### Limitations of BOW model\n",
    "\n",
    "Although bag-of-words is simple, easy to implement and in some applications works quite well, it has some serious limitations. The first one is the one is that the word order does not matter. If the purpose of the model is to classify texts based on some keywords then the word order may not be important. However, it's quite tricky - two single words may have very different meaning when they occur together. Let's take a closer look at this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bfbc1b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['actually', 'bad', 'good', 'movie', 'not', 'quite', 'the', 'was']\n",
      "The movie was not bad, actually quite good!\n",
      "[[1 1 1 1 1 1 1 1]]\n",
      "The movie was not good, actually quite bad!\n",
      "[[1 1 1 1 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "# Let's say there are two reviews of the same movie \n",
    "corpus = [\"The movie was not bad, actually quite good!\", \"The movie was not good, actually quite bad!\"]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(corpus)  # vectorizes learns numbers of word occurences (features)\n",
    "print(vectorizer.get_feature_names())\n",
    "for sentence in corpus:\n",
    "    print(sentence)\n",
    "    print(vectorizer.transform([sentence]).toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e022e851",
   "metadata": {},
   "source": [
    "Although those reviews express completely opposite emotions, both senteces have been represented in the exactly same way. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f9dbfb",
   "metadata": {},
   "source": [
    "The next problem is new words. What happens if we ask our model to vectorise a text which contains previously unseen words (i.e. those words weren't present in the training corpus)? We cannot add them to the corpus and encode them on the fly since this will change the length of the vector. The only reasonable solution is to dismiss all words which were not in the training corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9c231312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apples', 'bananas', 'like', 'they', 'we']\n",
      "We like apples, bananas\n",
      "[[1 1 1 0 1]]\n",
      "We like apples, bananas, plums\n",
      "[[1 1 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "corpus = [\"They like bananas\", \"We like apples\"]\n",
    "\n",
    "test_sentence1 = \"We like apples, bananas\"\n",
    "test_sentence2 = \"We like apples, bananas, plums\"\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(corpus)  # vectorizes learns numbers of word occurrences (features)\n",
    "print(vectorizer.get_feature_names())\n",
    "\n",
    "print(test_sentence1)\n",
    "print(vectorizer.transform([test_sentence1]).toarray())\n",
    "print(test_sentence2)\n",
    "print(vectorizer.transform([test_sentence2]).toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97df3bf1",
   "metadata": {},
   "source": [
    "As you can see, both sentences got encoded to the same vector. Because of this, we lost the additional information from the second sentence that \"they\" also like plums. The solution for this problem is to use sufficiently large training corpora. But it is inevitable that some words will be missed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d761a90a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
